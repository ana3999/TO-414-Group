---
title: "Project#3: Prediction"
author: "Group 1"
date: "3/29/2021"
output: html_document
---

```{r, include=FALSE}
# Required packages
library(tidyverse)
library(class)
library(caret)
library(gmodels)
library(neuralnet)
library(lmtest)
library(aod)
library(VGAM)
library(e1071)
library(C50)
```

### Data Set-up

Reading and Cleaning data: 
```{r}
resume <- read.csv("ResumeNames.csv", stringsAsFactors = TRUE)
resume$name <- NULL
resume$X <- NULL
resume$minimum <- as.integer(resume$minimum)
str(resume)
```

## Introduction

This data set contains different features from 4,870 resumes. The response variable for this data set is whether a candidate was called back for an interview or not. 
An important issue to note before beginning our analysis is the imbalance within the data set. With the nature of the hiring process, most candidates will not be called back for an interview. Thus, the response variable is extremely imbalanced, with 4478 "no" outcomes, and only 392 "yes" outcomes. To compensate for this imbalance, the split for the testing and training data sets were lowered to a ratio of 30:70, rather than 20:80. Moreover, within the prediction models, cut-offs were also lowered to about 0.15-0.20, instead of exactly 0.5. 

#### Creating Test and Train data sets (30:70 split)

Logistic Regression: no dummy variables or normalization
```{r}
set.seed(12345) # Random sample
testrows <- sample(1:nrow(resume),0.3*nrow(resume))
resumetest <- resume[testrows, ]
resumetrain <- resume[-testrows,]
```

Converting and Normalizing the data
```{r}
# Converting factors to dummy variables
resumemm <- as.data.frame(model.matrix(~.-1,resume))
# Normalizing dataset
normalize <- function(x) {return((x - min(x)) / (max(x) - min(x)))}
resumenorm <- as.data.frame(lapply(resumemm, normalize))
```

Linear Regression, Decision Tree, and ANN
```{r}
set.seed(12345) # Random sample
testrowsnorm <- sample(1:nrow(resumenorm), 0.3*nrow(resumenorm))
resumetestnorm <- resumenorm[testrowsnorm,]
resumetrainnorm <- resumenorm[-testrowsnorm,]
```

KNN
```{r}
# Predictors: all columns except "callyes"
resumetestknn <- resumenorm[testrowsnorm, -match("callyes",names(resumenorm))]
resumetrainknn <- resumenorm[-testrowsnorm, -match("callyes",names(resumenorm))]
# Response (Labels): only "callyes" column
resumetestknnlabels <- resumenorm[testrows, "callyes"]
resumetrainknnlabels <- resumenorm[-testrows, "callyes"]
```

## Linear Regression

Model 1: All variables
```{r}
lin <- glm(callyes ~ ., data = resumetrainnorm, family = "binomial")
summary(linreg)
```

```{r, include=FALSE}
steplin <- step(lin)
```

Model 2: Only significant variables (found from step function)
```{r}
summary(steplin)
```

### Prediction
```{r}
steplinpred <- ifelse(predict(steplin, newdata = resumetestnorm, type = "response") > 0.15, "1", "0")
confusionMatrix(as.factor(steplinpred), as.factor(resumetestnorm$call), positive="1")
```

## Logistic Regression

Model 1: All variables
```{r}
log <- glm(call ~ ., data = resumetrain, family = "binomial")
summary(logreg)
```

```{r, include=FALSE}
steplog <- step(log)
```

Model 2: Only significant variables (found from step function)
```{r}
summary(steplog)
```

### Prediction
```{r}
steplogpred <- ifelse(predict(steplog, newdata = resumetest, type = "response") > 0.2, "yes", "no")
confusionMatrix(as.factor(steplogpred), as.factor(resumetest$call))
```

## Decision Tree Model
```{r}
namesModel <- C5.0(as.factor(callyes) ~ ., data = resume_train_2)
summary(namesModel)
plot(namesModel)
#Predict test data
CallPrediction <- predict(namesModel, resume_test_2)
CrossTable(res_test_labels, CallPrediction)
confusionMatrix(data = as.factor(CallPrediction), reference = as.factor(res_test_labels))
#Improved Model
improvedNamesModel <- C5.0(as.factor(callyes) ~ ethnicitycauc + citychicago + jobs + experience + honorsyes + holesyes + emailyes + specialyes + collegeyes + equalyes + reqeducyes + reqcompyes + reqorgyes + industryfinance.insurance.real.estate + industryhealth.education.social.services + industrymanufacturing + industrytrade + industrytransport.communication + industryunknown , data = resume_test_2)
CallPrediction2 <- predict(improvedNamesModel, resume_test_2)
CrossTable(res_test_labels, CallPrediction2)
confusionMatrix(data = as.factor(CallPrediction2), reference = as.factor(res_test_labels))
```

## KNN Model
```{r}
knum <- sqrt(nrow(resume_train))
knn_pred <- knn(train = resume_train, test = resume_test, cl = res_train_labels, k=knum)
#summary(resume_train)
```
```{r}
library(gmodels)
CrossTable(x = res_test_labels, y = knn_pred, 
           prop.chisq=FALSE)
caret::confusionMatrix(data = as.factor(knn_pred), reference = as.factor(res_test_labels))
```

## ANN Model
```{r}
ANN_model1 <- neuralnet(callyes ~ . , data = resume_train_2, hidden = 1)

ANN_modelresults <- compute(ANN_model1, resume_test_2)
ANN_pred <- ANN_modelresults$net.result
resume_test_2$ANN_preds <- as.numeric(ifelse(ANN_pred > 0.20, 1, 0))

confusionMatrix(data = as.factor(resume_test_2$ANN_preds), reference = as.factor(resume_test_2$callyes), positive = '1')

str(resume_train_2)

ANN_model2 <- neuralnet(callyes ~ ethnicitycauc + citychicago + jobs + experience + honorsyes + holesyes + emailyes + specialyes + collegeyes + equalyes + reqeducyes + reqcompyes + reqorgyes + industryfinance.insurance.real.estate + industryhealth.education.social.services + industrymanufacturing + industrytrade + industrytransport.communication + industryunknown , data = resume_train_2, hidden = 1)

ANN_model2results <- compute(ANN_model2, resume_test_2)
ANN_pred2 <- ANN_model2results$net.result
resume_test_2$ANN_preds <- as.numeric(ifelse(ANN_pred2 > 0.20, 1, 0))

confusionMatrix(data = as.factor(resume_test_2$ANN_preds), reference = as.factor(resume_test_2$callyes), positive = '1')
```

## Data Frame with Predictions
```{r}
comb_pred <- data.frame(knn2 = knn_pred, log2 = log2pred, ann2 = ANN_pred2, dec_tree = CallPrediction2, callyes = res_test_labels)

test_rows_3 <- sample(1:nrow(comb_pred),0.3*nrow(comb_pred))
resume_test_4 <- comb_pred[test_rows_3,]
resume_train_4 <- comb_pred[-test_rows_3,]
comb_train_labels <- comb_pred[-test_rows_3, "callyes"]
comb_test_labels <- comb_pred[test_rows_3, "callyes"]
tree_model_2 <- C5.0(as.factor(callyes) ~ knn2 + log2 + ann2 + dec_tree, data = resume_train_4)
tree_pred_2 <- predict(tree_model_2, resume_test_4)

caret::confusionMatrix(data = as.factor(tree_pred_2), reference = as.factor(comb_test_labels))
```

## Conclusion

* List confusionmatrix summaries
* Identify most accurate model
* Identify and explain which variables were most significant


PAST WORK

```{r}
## Make all factors dummy variables
resumemm <- as.data.frame(model.matrix(~.-1, resume))
str(resumemm)
# Randomize the rows in the data (shuffling the rows)
set.seed(123)
res_random <- resumemm[sample(nrow(resumemm)),]
# #Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
# # we are going to normalize everything 
resume_norm <- as.data.frame(lapply(res_random, normalize))
##resume_z <- as.data.frame(scale(res_random[-1]))
```

```{r}
## Train and test sets for KNN
testrows <- sample(1:nrow(resume_norm), 0.3*nrow(resume_norm))
resume_train <- resume_norm[-testrows,-match("callyes",names(resume_norm))]
resume_test <- resume_norm[testrows, -match("callyes",names(resume_norm))]

## Train and test sets for other models
resume_train_2 <- resume_norm[-testrows,]
resume_test_2 <- resume_norm[testrows,]

#Now the response (aka Labels) - only the callyes column
res_train_labels <- resume_norm[-testrows, "callyes"]
res_test_labels <- resume_norm[testrows, "callyes"]

## Train and test sets without dummy variables and normalization
testrows_2 <- sample(1:nrow(resume), 0.3*nrow(resume))
resume_test_3 <- resume[testrows, ]
```
